{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136, 31714)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\U108-N257\\AppData\\Local\\Temp\\ipykernel_36720\\3332090209.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_markers = pd.read_csv('data/markers.csv', header=[0], index_col=0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import pearsonr\n",
    "from json import dump\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "##  DATA LOADING  ##\n",
    "\n",
    "# load parameters\n",
    "df_params = pd.read_csv('data/parameters.csv', header=[0], index_col=0)\n",
    "parameters_raw = np.array([[df_params.loc[i][j] for j in df_params.columns] for i in df_params.index])\n",
    "y_scalers = [MinMaxScaler() for _ in range(parameters_raw.shape[1])]\n",
    "parameters = np.zeros_like(parameters_raw)\n",
    "for i in range(parameters_raw.shape[1]):\n",
    "    parameters[:, i] = y_scalers[i].fit_transform(parameters_raw[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "# load identifiers (vertical headers)\n",
    "identifiers = df_params.index.values\n",
    "\n",
    "# load markers\n",
    "df_markers = pd.read_csv('data/markers.csv', header=[0], index_col=0)\n",
    "df_markers = df_markers.transpose()\n",
    "print(df_markers.shape)\n",
    "markers_raw = np.array([df_markers.loc[i].values[1:].astype(float) for i in df_markers.index])\n",
    "x_scaler = MinMaxScaler()\n",
    "\n",
    "# scale the markers (apply the scaler globally not per columns)\n",
    "flattened_markers = markers_raw.ravel()\n",
    "scaled_markers = x_scaler.fit_transform(flattened_markers.reshape(-1, 1)).ravel()\n",
    "markers = scaled_markers.reshape(markers_raw.shape)\n",
    "\n",
    "# extract traits\n",
    "traits = np.array([df_markers.loc[i].values[0] for i in df_markers.index])\n",
    "df_params['trait'] = traits\n",
    "trait_scaler = MinMaxScaler()\n",
    "traits = trait_scaler.fit_transform(traits.reshape(-1, 1)).flatten()\n",
    "y_scalers.append(trait_scaler)\n",
    "parameters = np.column_stack((parameters, traits))\n",
    "\n",
    "\n",
    "# Generate folds - either from R analyses or generated in python\n",
    "def generate_folds(load_folds=False, num_repeats=2, num_folds=5, seed=42):\n",
    "    if load_folds:\n",
    "        ####### Load R Folds #######\n",
    "        df_fold = pd.read_csv('data/folds_bayes_lasso.csv', header=[0], index_col=0)\n",
    "        folds = np.array(df_fold.iloc[:, 0])\n",
    "        num_repeats = 1\n",
    "        num_folds = 5\n",
    "        data_folds = {}\n",
    "        for fold in range(1, num_folds + 1):\n",
    "            test_idx = np.where(folds == fold)[0]\n",
    "            train_idx = np.where(folds != fold)[0]\n",
    "\n",
    "            test_data = {\n",
    "                'indices': test_idx,\n",
    "                'identifiers': identifiers[test_idx],\n",
    "                'parameters': parameters[test_idx],\n",
    "                'markers': markers[test_idx],\n",
    "                'trait': traits[test_idx],\n",
    "            }\n",
    "\n",
    "            train_data = {\n",
    "                'indices': train_idx,\n",
    "                'identifiers': identifiers[train_idx],\n",
    "                'parameters': parameters[train_idx],\n",
    "                'markers': markers[train_idx],\n",
    "                'trait': traits[train_idx],\n",
    "            }\n",
    "\n",
    "            data_folds[fold] = {\n",
    "                'train': train_data,\n",
    "                'test': test_data\n",
    "            }\n",
    "    else:\n",
    "        ####### Generate repeated K-Folds #######\n",
    "        num_samples = len(identifiers)\n",
    "        data_folds = {}\n",
    "        fold_indices = pd.DataFrame(identifiers, columns=['identifier'])\n",
    "        for repeat_num in range(1, num_repeats + 1):\n",
    "            fold_indices[repeat_num] = np.nan\n",
    "            \n",
    "\n",
    "        rkf = RepeatedKFold(n_splits=num_folds, n_repeats=num_repeats, random_state=seed)\n",
    "\n",
    "        for repeat, (train_index, test_index) in enumerate(rkf.split(identifiers), start=1):\n",
    "            fold = (repeat - 1) % num_folds + 1\n",
    "            repeat_num = (repeat - 1) // num_folds + 1\n",
    "\n",
    "            test_data = {\n",
    "                'indices': test_index,\n",
    "                'identifiers': identifiers[test_index],\n",
    "                'parameters': parameters[test_index],\n",
    "                'markers': markers[test_index],\n",
    "                'trait': traits[test_index],\n",
    "            }\n",
    "\n",
    "            train_data = {\n",
    "                'indices': train_index,\n",
    "                'identifiers': identifiers[train_index],\n",
    "                'parameters': parameters[train_index],\n",
    "                'markers': markers[train_index],\n",
    "                'trait': traits[train_index],\n",
    "            }\n",
    "\n",
    "            data_folds[(repeat_num, fold)] = {\n",
    "                'train': train_data,\n",
    "                'test': test_data,\n",
    "                'repeat': repeat_num,\n",
    "            }\n",
    "            \n",
    "            fold_indices.loc[test_index, repeat_num] = fold\n",
    "\n",
    "        fold_indices.to_csv('generated/fold_indices.csv', index=False)\n",
    "    \n",
    "    return data_folds\n",
    "\n",
    "\n",
    "# Metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    pearson, _ = pearsonr(y_true, y_pred)\n",
    "    return mse, rmse, mae, r2, pearson\n",
    "\n",
    "# Sort results\n",
    "def sort_dict_recursively(d):\n",
    "    if isinstance(d, dict):\n",
    "        # Sort keys: integers before strings\n",
    "        sorted_items = sorted(d.items(), key=lambda x: (isinstance(x[0], str), x[0]))\n",
    "        return OrderedDict(\n",
    "            (k, sort_dict_recursively(v)) for k, v in sorted_items\n",
    "        )\n",
    "    elif isinstance(d, list):\n",
    "        # Apply the function recursively to each item in the list\n",
    "        return [sort_dict_recursively(i) for i in d]\n",
    "    else:\n",
    "        # Return the value as is if it's neither a dictionary nor a list\n",
    "        return d\n",
    "    \n",
    "\n",
    "def nested_defaultdict():\n",
    "    return defaultdict(nested_defaultdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1_1 - bacon - Epsib > "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 116\u001b[0m\n\u001b[0;32m    114\u001b[0m     auto_save \u001b[38;5;241m=\u001b[39m Auto_Save(model_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m param_key, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    115\u001b[0m     lrScheduler \u001b[38;5;241m=\u001b[39m LearningRateScheduler(clr)\n\u001b[1;32m--> 116\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_markers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlrScheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_save\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_markers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_parameters\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_markers, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Compute metrics\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Workspace\\ML\\TF210_ENV\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Workspace\\ML\\TF210_ENV\\lib\\site-packages\\keras\\engine\\training.py:1555\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m   1552\u001b[0m     data_handler\u001b[38;5;241m.\u001b[39m_initial_step \u001b[38;5;241m=\u001b[39m data_handler\u001b[38;5;241m.\u001b[39m_initial_step \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1553\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_load_initial_step_from_ckpt()\n\u001b[0;32m   1554\u001b[0m     )\n\u001b[1;32m-> 1555\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[0;32m   1556\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m             epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m             _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m         ):\n\u001b[0;32m   1563\u001b[0m             callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n",
      "File \u001b[1;32mc:\\Workspace\\ML\\TF210_ENV\\lib\\site-packages\\keras\\engine\\data_adapter.py:1374\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[0;32m   1373\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1374\u001b[0m original_spe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m   1375\u001b[0m can_run_full_execution \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1376\u001b[0m     original_spe \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m original_spe\n\u001b[0;32m   1379\u001b[0m )\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[1;32mc:\\Workspace\\ML\\TF210_ENV\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:637\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    636\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    638\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    639\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Workspace\\ML\\TF210_ENV\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32mc:\\Workspace\\ML\\TF210_ENV\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1122\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1124\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "import catboost as cb\n",
    "\n",
    "# Deep Learning Models\n",
    "\n",
    "from tensorflow_utils import cnn_softmax, cnn_bacon, cnn_decon, CNN_transformer, depth_res, transformer_model\n",
    "from tensorflow_utils import Auto_Save, clr\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, LearningRateScheduler\n",
    "\n",
    "\n",
    "# Suppress specific warning from PLS\n",
    "warnings.filterwarnings(\"ignore\", message=\"Y residual is constant at iteration\")\n",
    "\n",
    "# the list of models to be trained\n",
    "ml_models = [\n",
    "    ('knn', KNeighborsRegressor(leaf_size= 71, n_neighbors= 9, weights= 'distance', p= 2)),\n",
    "    ('RF', RandomForestRegressor(bootstrap=False, max_depth=87, max_features=\"sqrt\", min_samples_leaf=2, min_samples_split=6, n_estimators=318)),\n",
    "    ('svm', SVR(kernel='rbf', epsilon=0.02604615195102571, C=1)),\n",
    "    ('ElasticNet', ElasticNet(alpha=5.09563321426854, l1_ratio=2.5128617959953626e-08, max_iter=20000, tol=0.06185856629377155, selection='cyclic')),\n",
    "    ('PLS', PLSRegression(n_components=100)),\n",
    "    ('BayesianRidge', BayesianRidge(max_iter=1000, tol=0.0001)),\n",
    "    ('Ridge', Ridge(alpha=82.61743872370975, solver='saga', max_iter=20000, tol=0.02531618477469109)),\n",
    "    ('Lasso', Lasso(alpha=0.0017742588786429565, tol=0.002395379538656458, max_iter=40000)),\n",
    "    ('XGBoost', XGBRegressor(n_estimators=970, max_depth=21, min_child_weight=3.8901787266768046, reg_lambda=1.2476564637797165, alpha=0.00015777908963872925, subsample=0.514174538559911, colsample_bytree=0.8748701490064003, eta=0.9436165548239603, gamma=0.050764442960315445, learning_rate=0.0003697533172127718, booster='gblinear', verbosity=0)),\n",
    "    ('cnn_softmax', cnn_softmax(markers.shape[1]), 100),\n",
    "    ('bacon', cnn_bacon(markers.shape[1]), 100),\n",
    "    # ('AdaBoost', AdaBoostRegressor(n_estimators=55, learning_rate=0.00465096494044598, loss='exponential')),\n",
    "    # ('CatBoost', cb.CatBoostRegressor(learning_rate=0.016588626830975256, depth=6, subsample=0.33380061672814154, colsample_bylevel=0.5087556106888949, min_data_in_leaf=35, silent=True)),\n",
    "    \n",
    "    # ('CNN_transformer', CNN_transformer(markers.shape[1]), 100),\n",
    "    # ('decon', cnn_decon(markers.shape[1]), 100),\n",
    "    # # ('depth_res', depth_res(markers.shape[1]), 50),\n",
    "    # # ('transformer', transformer_model(markers.shape[1]), 20),\n",
    "]\n",
    "\n",
    "\n",
    "num_repeats = 15\n",
    "num_folds = 5\n",
    "load_folds = False # Set to True to load R folds, False to generate new folds with num_repeats and num_folds\n",
    "seed = 42\n",
    "\n",
    "# PREPARE DATA\n",
    "data_folds = generate_folds(load_folds, num_repeats, num_folds, seed)\n",
    "\n",
    "fold_indices = []\n",
    "if load_folds:\n",
    "    fold_indices = [(fold, str(fold)) for fold in range(1, num_folds + 1)]\n",
    "else:\n",
    "    fold_indices = [((repeat, fold), f\"{repeat}_{fold}\") for repeat in range(1, num_repeats + 1) for fold in range(1, num_folds + 1)]\n",
    "\n",
    "\n",
    "results = {}\n",
    "all_results_in_one = nested_defaultdict()\n",
    "all_results_in_one[\"seed\"] = seed\n",
    "\n",
    "\n",
    "# TRAIN MODELS\n",
    "for fold, foldname in fold_indices:\n",
    "    train_data = data_folds[fold]['train']\n",
    "    test_data = data_folds[fold]['test']\n",
    "\n",
    "    train_identifiers = train_data['identifiers']\n",
    "    train_parameters = train_data['parameters']\n",
    "    train_markers = train_data['markers']\n",
    "    train_trait = train_data['trait']\n",
    "\n",
    "    test_identifiers = test_data['identifiers']\n",
    "    test_parameters = test_data['parameters']\n",
    "    test_markers = test_data['markers']\n",
    "    test_trait = test_data['trait']\n",
    "\n",
    "    if fold not in results:\n",
    "        results[fold] = {}\n",
    "\n",
    "    for config in ml_models:\n",
    "        model_name = config[0]\n",
    "        model = config[1]\n",
    "        patience = config[2] if len(config) > 2 else None\n",
    "        \n",
    "        \n",
    "        if model_name not in results[fold]:\n",
    "            results[fold][model_name] = {}\n",
    "\n",
    "        for i in range(train_parameters.shape[1]):\n",
    "            param_key = df_params.columns[i]\n",
    "            y = train_parameters[:, i]\n",
    "            \n",
    "            print(f'Fold {foldname} - {model_name} - {param_key} >', end=' ')\n",
    "            \n",
    "            if param_key not in results[fold][model_name]:\n",
    "                results[fold][model_name][param_key] = {}\n",
    "\n",
    "            if patience is None:\n",
    "                model.fit(train_markers, y)\n",
    "                predictions = model.predict(test_markers).flatten()\n",
    "            else:\n",
    "                reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=patience)\n",
    "                early_stop = EarlyStopping(monitor=\"val_loss\", patience=patience*3, verbose=0, mode=\"min\", min_delta=0)\n",
    "                auto_save = Auto_Save(model_name + \"_\" + param_key, verbose=0)\n",
    "                lrScheduler = LearningRateScheduler(clr)\n",
    "                model.fit(train_markers, y, epochs=patience*10, verbose=0, callbacks=[lrScheduler, early_stop, auto_save], validation_data=(test_markers, test_parameters[:, i]))\n",
    "                predictions = model.predict(test_markers, verbose=0).flatten()\n",
    "                \n",
    "            \n",
    "            # Compute metrics\n",
    "            mse, rmse, mae, r2, pearson = calculate_metrics(test_parameters[:, i], predictions)\n",
    "            unscaled_preds = y_scalers[i].inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "            unscaled_true = y_scalers[i].inverse_transform(test_parameters[:, i].reshape(-1, 1)).flatten()\n",
    "            _, rmse_unscaled, _, _, _ = calculate_metrics(unscaled_true, unscaled_preds)\n",
    "\n",
    "\n",
    "            results[fold][model_name][param_key] = {\n",
    "                'RMSE': rmse,\n",
    "                'RMSE_unscaled': rmse_unscaled,\n",
    "                'MAE': mae,\n",
    "                'MSE': mse,\n",
    "                'R2': r2,\n",
    "                'pearson': pearson,\n",
    "                'predictions': unscaled_preds,\n",
    "            }            \n",
    "            print(f' RMSE: {rmse:.4f} - RMSE_unscaled: {rmse_unscaled:.4f} - MAE: {mae:.4f} - R2: {r2:.4f} - Pearson: {pearson:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GENERATE REPORTS\n",
    "keys = ['RMSE_unscaled', 'MAE', 'R2', 'pearson']\n",
    "param_count = len(df_params.columns)\n",
    "\n",
    "repeated_metrics = np.full((param_count + 4, len(ml_models) * len(keys) * num_repeats * num_folds + 1), np.nan).tolist()\n",
    "for model_index, model_tuple in enumerate(ml_models):\n",
    "    model_name = model_tuple[0]\n",
    "    for key_index, key in enumerate(keys):\n",
    "        for repeat_index in range(num_repeats):\n",
    "            for fold_index in range(num_folds):\n",
    "                repeated_metrics[0][ (key_index * len(ml_models) * num_repeats * num_folds) + (repeat_index * len(ml_models) * num_folds) + (fold_index * len(ml_models)) + model_index + 1 ] = key\n",
    "                repeated_metrics[1][ (key_index * len(ml_models) * num_repeats * num_folds) + (repeat_index * len(ml_models) * num_folds) + (fold_index * len(ml_models)) + model_index + 1 ] = f'Repeat {repeat_index + 1}'\n",
    "                repeated_metrics[2][ (key_index * len(ml_models) * num_repeats * num_folds) + (repeat_index * len(ml_models) * num_folds) + (fold_index * len(ml_models)) + model_index + 1 ] = f'Fold {fold_index + 1}'\n",
    "                repeated_metrics[3][ (key_index * len(ml_models) * num_repeats * num_folds) + (repeat_index * len(ml_models) * num_folds) + (fold_index * len(ml_models)) + model_index + 1 ] = model_name\n",
    "for param_index, param_key in enumerate(df_params.columns):\n",
    "    repeated_metrics[param_index + 4][0] = param_key\n",
    "    \n",
    "\n",
    "repeated_predictions = np.full((len(identifiers) + 3, len(ml_models) * param_count * num_repeats + 1), np.nan).tolist()\n",
    "for model_index, model_tuple in enumerate(ml_models):\n",
    "    model_name = model_tuple[0]\n",
    "    for param_index, param_key in enumerate(df_params.columns):\n",
    "        for repeat_index in range(num_repeats):\n",
    "            repeated_predictions[0][model_index * param_count * num_repeats + param_index * num_repeats + repeat_index + 1] = model_name\n",
    "            repeated_predictions[1][model_index * param_count * num_repeats + param_index * num_repeats + repeat_index + 1] = param_key\n",
    "            repeated_predictions[2][model_index * param_count * num_repeats + param_index * num_repeats + repeat_index + 1] = f'Repeat {repeat_index + 1}'\n",
    "for idx, id in enumerate(identifiers):\n",
    "    repeated_predictions[idx + 3][0] = id\n",
    "\n",
    "\n",
    "mean_metrics = np.full((param_count + 2, len(ml_models) * len(keys) + 1), 0).tolist()\n",
    "for model_index, model_tuple in enumerate(ml_models):\n",
    "    model_name = model_tuple[0]\n",
    "    for key_index, key in enumerate(keys):\n",
    "        mean_metrics[0][key_index * len(ml_models) + model_index + 1] = key\n",
    "        mean_metrics[1][key_index * len(ml_models) + model_index + 1] = model_name\n",
    "for param_index, param_key in enumerate(df_params.columns):\n",
    "    mean_metrics[param_index + 2][0] = param_key\n",
    "\n",
    "\n",
    "mean_predictions = np.full((len(identifiers) + 2, len(ml_models) * param_count + 1), 0).tolist()\n",
    "for model_index, model_tuple in enumerate(ml_models):\n",
    "    model_name = model_tuple[0]\n",
    "    for param_index, param_key in enumerate(df_params.columns):\n",
    "        mean_predictions[0][param_index * len(ml_models) + model_index + 1] = param_key\n",
    "        mean_predictions[1][param_index * len(ml_models) + model_index + 1] = model_name\n",
    "for idx, id in enumerate(identifiers):\n",
    "    mean_predictions[idx + 2][0] = id\n",
    "\n",
    "# FILL REPORTS\n",
    "for fold, fold_results in results.items():\n",
    "    repeat_index = fold[0] if isinstance(fold, tuple) else 0\n",
    "    fold_index = fold[1] if isinstance(fold, tuple) else fold\n",
    "    sample_indices = data_folds[(repeat_index, fold_index)]['test']['indices']\n",
    "    prediction_indices = sample_indices\n",
    "    prediction_indices = prediction_indices.tolist()\n",
    "    fold_index = fold_index - 1\n",
    "    repeat_index = repeat_index - 1\n",
    "    all_results_in_one[\"indices\"][repeat_index][fold_index] = sample_indices.tolist()\n",
    "    \n",
    "    for model_index, (model_name, model_results) in enumerate(fold_results.items()):\n",
    "        for param_index, (param_key, param_results) in enumerate(model_results.items()):\n",
    "            for key_index, key in enumerate(keys):\n",
    "                repeated_metrics[param_index + 4][(key_index * len(ml_models) * num_repeats * num_folds) + (repeat_index * len(ml_models) * num_folds) + (fold_index * len(ml_models)) + model_index + 1] = param_results[key]\n",
    "                mean_metrics[param_index + 2][(key_index * len(ml_models)) + model_index + 1] += param_results[key] / (num_folds * num_repeats)\n",
    "                all_results_in_one[model_name][param_key][key][repeat_index][fold_index] = param_results[key]\n",
    "                all_results_in_one[model_name][param_key][key][\"mean\"] = mean_metrics[param_index + 2][(key_index * len(ml_models)) + model_index + 1]\n",
    "                \n",
    "            for row, src_index in enumerate(prediction_indices):\n",
    "                repeated_predictions[src_index + 3][model_index * param_count * num_repeats + param_index * num_repeats + repeat_index + 1] = param_results['predictions'][row]\n",
    "                mean_predictions[src_index + 2][param_index * len(ml_models) + model_index + 1] += param_results['predictions'][row] / num_repeats\n",
    "                \n",
    "                all_results_in_one[model_name][param_key][\"predictions\"][repeat_index][identifiers[src_index]] = param_results['predictions'][row]\n",
    "                all_results_in_one[model_name][param_key][\"predictions_mean\"][identifiers[src_index]] = mean_predictions[src_index + 2][param_index * len(ml_models) + model_index + 1]\n",
    "\n",
    "csv_predictions = pd.DataFrame(repeated_predictions)\n",
    "csv_predictions.to_csv('reports/predictions.csv', index=False, header=False)\n",
    "\n",
    "csv_metrics = pd.DataFrame(repeated_metrics)\n",
    "csv_metrics.to_csv('reports/metrics.csv', index=False, header=False)\n",
    "\n",
    "csv_mean_predictions = pd.DataFrame(mean_predictions)\n",
    "csv_mean_predictions.to_csv('reports/mean_predictions.csv', index=False, header=False)\n",
    "\n",
    "csv_mean_metrics = pd.DataFrame(mean_metrics)\n",
    "csv_mean_metrics.to_csv('reports/mean_metrics.csv', index=False, header=False)\n",
    "\n",
    "all_results_in_one = sort_dict_recursively(all_results_in_one)\n",
    "json.dump(all_results_in_one, open('reports/all_results_in_one.json', 'w'), indent=4)\n",
    "\n",
    "\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, np.float32):\n",
    "        return float(obj)\n",
    "    raise TypeError(f\"Object of type {obj.__class__.__name__} is not JSON serializable\")\n",
    "\n",
    "\n",
    "# all_results_in_one = json.dumps(all_results_in_one, default=convert_to_serializable)\n",
    "# all_results_in_one = sort_dict_recursively(all_results_in_one)\n",
    "# # print(all_results_in_one)\n",
    "# json.dump(all_results_in_one, open('all_results_in_one.json', 'w'), indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinard_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
